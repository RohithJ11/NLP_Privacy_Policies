{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmCFT6K4FS6TFjn0rIOCKa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RohithJ11/NLP_Privacy_Policies/blob/main/Text_to_Text_Transfer_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### installing TensorFlow and the Transformers libraries"
      ],
      "metadata": {
        "id": "sDCr0PImDMz1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1SyTsgJrsyh",
        "outputId": "3acc9c35-a343-4d59-c7c8-75886169e3be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.10.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing nltk library (Natural Language Toolkit)"
      ],
      "metadata": {
        "id": "bV5auFrWD0Yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lnac3vUMuDuC",
        "outputId": "53fdbc41-9168-4bc0-e80b-60a695277b6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining and Applying the Keyword Extraction Function"
      ],
      "metadata": {
        "id": "EGETizBvCrxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.probability import FreqDist\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function to extract keywords from text based on frequency\n",
        "def extract_keywords(text, top_n=5):\n",
        "    # Tokenize the words\n",
        "    words = nltk.word_tokenize(text)\n",
        "    # Calculate frequency distribution\n",
        "    freq_dist = FreqDist(words)\n",
        "    # Extract the top n keywords\n",
        "    keywords = [word for word, freq in freq_dist.most_common(top_n)]\n",
        "    return keywords\n",
        "\n",
        "# Ensure this line is after the 'nltk' and function definitions\n",
        "df['Keywords'] = df['Summary'].apply(extract_keywords)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDOc_lH5uM2o",
        "outputId": "df86b16e-ed1a-44fb-8805-50c860a68bd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### loading dataset by using the T5 model to generate summaries for the text in dataset, and applying a simple frequency-based method to extract keywords"
      ],
      "metadata": {
        "id": "Ih20WQHrEn5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/Privacyplcy_DS_3.csv')  # Adjust the path as necessary\n",
        "\n",
        "# Initialize the model and tokenizer\n",
        "model_name = 't5-small'  # You can choose other versions like 't5-base' or 't5-large' depending on your resource availability\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Function to summarize text\n",
        "def summarize_text(text):\n",
        "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    summary_ids = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Apply summarization to each row in the DataFrame\n",
        "df['Summary'] = df['Summary_of_Content'].apply(summarize_text)  # Assuming the text column is named 'PolicyText'\n",
        "\n",
        "# Function to extract keywords from text based on frequency\n",
        "def extract_keywords(text, top_n=5):\n",
        "    words = text.split()\n",
        "    freq_dist = nltk.FreqDist(words)\n",
        "    keywords = [word for word, freq in freq_dist.most_common(top_n)]\n",
        "    return keywords\n",
        "\n",
        "# Apply keyword extraction on the summaries\n",
        "df['Keywords'] = df['Summary'].apply(extract_keywords)\n",
        "\n",
        "# Display the DataFrame to verify the results\n",
        "#print(df[['Summary', 'Keywords']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gf-IkEQVr1om",
        "outputId": "b8b4a9f2-63aa-4336-b58c-f86b9655708f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              Summary  \\\n",
            "0   Directly collected data includes personal info...   \n",
            "1   Pfizer's use of personal data is detailed acro...   \n",
            "2   this section elucidates how Pfizer may share p...   \n",
            "3   users have the right to request access to, cor...   \n",
            "4   Pfizer and its partners use cookies, tracking ...   \n",
            "5   Pfizer employs technical, administrative, and ...   \n",
            "6   users have the ability to update their persona...   \n",
            "7   Pfizer will retain users' personal data for as...   \n",
            "8   the section outlines the scope and limitations...   \n",
            "9   the types of information collected and receive...   \n",
            "10  Slack processes Customer Data according to a c...   \n",
            "11  customers, administrators, and authorized user...   \n",
            "12  Slack retains Customer Data based on the custo...   \n",
            "13  Slack holds international recognized security ...   \n",
            "14  individuals in the european economic area, the...   \n",
            "15  Epson collects personal information from users...   \n",
            "16  the section also directs users to additional i...   \n",
            "17  the section elaborates on the integration of s...   \n",
            "18  companies such as Yahoo, Microsoft/Bing, Faceb...   \n",
            "19  users can access, update, or remove certain in...   \n",
            "20  parents or guardians concerned about their chi...   \n",
            "21  participants can enter sweepstakes, contests, ...   \n",
            "\n",
            "                                             Keywords  \n",
            "0          [data, personal, the, Directly, collected]  \n",
            "1                      [and, data, Pfizer's, use, of]  \n",
            "2            [Pfizer, personal, data, business, this]  \n",
            "3                    [personal, by, users, have, the]  \n",
            "4                    [and, to, Pfizer, its, partners]  \n",
            "5           [Pfizer, and, unauthorized, or, security]  \n",
            "6                     [users, have, the, ability, to]  \n",
            "7                        [the, Pfizer, for, as, will]  \n",
            "8                         [of, the, and, to, Slack's]  \n",
            "9           [and, information,, the, of, information]  \n",
            "10             [to, Slack, processes, Customer, Data]  \n",
            "11           [users, or, authorized, to, information]  \n",
            "12              [and, Slack, retains, Customer, Data]  \n",
            "13  [security, Slack, holds, international, recogn...  \n",
            "14           [the, to, individuals, rights, personal]  \n",
            "15         [data,, information, and, Epson, collects]  \n",
            "16               [the, regarding, their, of, section]  \n",
            "17                      [the, of, social, media, and]  \n",
            "18             [to, advertising, companies, such, as]  \n",
            "19             [users, can, or, information, through]  \n",
            "20          [information, the, of, personal, parents]  \n",
            "21                  [the, participants, and, to, can]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the output with summaries and keywords into a CSV file which cab be downloaded"
      ],
      "metadata": {
        "id": "SrdU3vuaEwXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the DataFrame with summaries and keywords to a new CSV file\n",
        "output_filename = '/content/privacy_policy_summaries_keywords.csv'\n",
        "df.to_csv(output_filename, index=False)\n",
        "\n",
        "# Inform the user about the saved file\n",
        "print(f'Saved the output to {output_filename}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0W_Fi1Ykv4d3",
        "outputId": "32ed0db6-2fbc-4524-c92e-828bc35239cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved the output to /content/privacy_policy_summaries_keywords.csv\n"
          ]
        }
      ]
    }
  ]
}